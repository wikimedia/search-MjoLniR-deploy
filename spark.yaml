# Configuration shared by all training groups and
working_dir: /srv/deployment/search/mjolnir
global:
    environment:
        PYSPARK_PYTHON: venv/bin/python
        SPARK_CONF_DIR: /etc/spark2/conf
        SPARK_HOME: "/usr/lib/spark2"
    template_vars:
        spark_version: 2.3.1
        master: yarn
        # Path to spark-submit applicatoin
        spark_submit: "%(SPARK_HOME)s/bin/spark-submit"
        # Local path to zip'd virtualenv which will be shipped to executors
        mjolnir_venv_zip: "%(working_dir)s/mjolnir_venv.zip"
        # Local path to python script for running mjolnir utilities
        mjolnir_utility_path: "%(working_dir)s/venv/bin/mjolnir-utilities.py"

        # Paths to our parent data directory
        data_path_prefix: "user/%(USER)s/mjolnir/%(marker)s"
        hdfs_data_path_prefix: "hdfs://analytics-hadoop/%(data_path_prefix)s"
        local_data_path_prefix: "/mnt/hdfs/%(data_path_prefix)s"

        # Path inside hdfs to the labeled data
        hdfs_labeled_data_path: "%(hdfs_data_path_prefix)s/labeled"
        local_labeled_data_path: "%(local_data_path_prefix)s/labeled"

        # Path inside hdfs to labeled data with features"
        hdfs_features_data_path: "%(hdfs_data_path_prefix)s/features"
        local_features_data_path: "%(local_data_path_prefix)s/features"

        # Path inside hdfs to training data after feature selection
        hdfs_pruned_data_path: "%(hdfs_data_path_prefix)s/pruned"
        local_pruned_data_path: "%(local_data_path_prefix)s/pruned"

        # Fully qualified HDFS path to pre-folded data
        fold_data_suffix: "-folds-%(profile_name)s"
        hdfs_fold_data_path: "%(hdfs_data_path_prefix)s/folded"
        local_fold_data_path: "%(local_data_path_prefix)s/folded"

        # Base directory used to build path to write training output to
        base_training_output_dir: "%(HOME)s/training_size"
        training_output_dir: "%(base_training_output_dir)s/%(marker)s-%(profile_name)s"

        # Brokers to use to bootstrap kafka connections
        kafka_brokers: kafka-jumbo1001.eqiad.wmnet:9092,kafka-jumbo1002.eqiad.wmnet:9092,kafka-jumbo1003.eqiad.wmnet:9092
        kafka_request_topic: mjolnir.msearch-prod-request
        kafka_result_topic: mjolnir.msearch-prod-response

        # Number of cpu cores to assign per task. Must be a multiple of
        # cores_per_executor. Spark can't take advantage of this being > 1, but
        # xgboost can.
        cores_per_task: 1
        # Number of cpu cores to request per executor. If cores_per_task is
        # less than this spark will run multiple tasks per executor in separate
        # threads.
        cores_per_executor: 1
        # Size of JVM heap on executors
        executor_memory: 2G
        # Additional memory allocated by yarn beyond executor_memory. This
        # accounts for off-heap data structures both in the JVM itself, and
        # those created via JNI for xgboost. Primarily this is set here so it
        # can be overridden from the command line.
        executor_memory_overhead: 512
        # Used by the data pipeline to decide the minimum number of sessions
        # to group together. Setting this too high on low volume wikis will
        # result in little to no training data.
        min_sessions_per_query: 10
        # Name of LTR feature set to collect features from
        feature_definitions: featureset:enwiki_v1
    # Files that must exist to run
    paths:
        dir_exist: !!set
            ? "%(SPARK_CONF_DIR)s"
        file_exist: !!set
            ? "%(mjolnir_venv_zip)s"
            ? "%(mjolnir_utility_path)s"
            ? "%(spark_submit)s"
            ? "%(PYSPARK_PYTHON)s"
    # Configuration for auto-detected values
    autodetect:
        # Set limits to use no more than 1TB of memory
        agg_memory_limit: 1T
        # limits to use no more than 1/3 of cluster cpu
        agg_cpu_limit: 450
        # max memory that can be assigned to single executor
        executor_max_memory: 50G
        # Amount of memory needed for pyspark/jvm
        baseline_memory_overhead: 512M
        # Number of bytes of memory overhead to allocate per
        # value of the feature matrix
        bytes_per_value:
            make_folds: 29
            train: 20
    spark_args:
        master: "%(master)s"
        # TODO: When is this necessary?
        files: /usr/lib/libhdfs.so.0.0.0
        # Ship the mjolnir virtualenv to executors and decompress it to ./venv
        archives: "%(mjolnir_venv_zip)s#venv"
        executor-cores: "%(cores_per_executor)s"
        executor-memory: "%(executor_memory)s"
        # Source our jvm dependencies from archiva. 
        repositories: https://archiva.wikimedia.org/repository/releases,https://archiva.wikimedia.org/repository/snapshots,https://archiva.wikimedia.org/repository/mirrored
        packages: ml.dmlc:xgboost4j-spark:0.8-wmf-2-SNAPSHOT,org.wikimedia.search:mjolnir:0.4-SNAPSHOT,org.apache.spark:spark-streaming-kafka-0-8_2.11:%(spark_version)s,sramirez:spark-infotheoretic-feature-selection:1.4.4,sramirez:spark-MDLP-discretization:1.4.1
    spark_conf:
        spark.task.cpus: "%(cores_per_task)s"
        spark.executor.memoryOverhead: "%(executor_memory_overhead)s"
        # While undesirable, we can't disable the public (maven central) repository
        # until spark 2.2, which depends on java 8 (and our cluster is on java 7 still)
        spark.driver.extraJavaOptions: "-Dhttp.proxyHost=webproxy.eqiad.wmnet -Dhttp.proxyPort=8080 -Dhttps.proxyHost=webproxy.eqiad.wmnet -Dhttps.proxyPort=8080"
    commands:
        pyspark:
            spark_command: "%(SPARK_HOME)s/bin/pyspark"
        # Shell used to test model training
        pyspark_train:
            spark_command: "%(SPARK_HOME)s/bin/pyspark"
            template_vars:
                cores_per_executor: 4
                cores_per_task: 4
                executor_memory_overhead: 6144
        data_pipeline:
            spark_command: "%(SPARK_HOME)s/bin/spark-submit"
            mjolnir_utility_path: "%(mjolnir_utility_path)s"
            mjolnir_utility: data_pipeline
            paths:
                dir_not_exist: !!set
                    ? "%(local_labeled_data_path)s"
            spark_args:
                # Ivy blows up if we try and use a package dependency. Pull only the jar
                # and not it's deps.
                jars: hdfs://analytics-hadoop/wmf/refinery/current/artifacts/refinery-hive.jar
            cmd_args:
                input: hdfs://analytics-hadoop/wmf/data/discovery/query_clicks/daily/year=*/month=*/day=*
                output-dir: "%(hdfs_labeled_data_path)s"
                # Maximum number of training observations per-wiki. We usually get a bit less
                # than requestsed, 35M turns into 25 or 30M.
                samples-per-wiki: 35000000
                kafka: "%(kafka_brokers)s"
                kafka-request-topic: "%(kafka_request_topic)"
                kafka-result-topic: "%(kafka_result_topic)"
                min-sessions: "%(min_sessions_per_query)s"
        collect_features:
            spark_command: "%(SPARK_HOME)s/bin/spark-submit"
            mjolnir_utility_path: "%(mjolnir_utility_path)s"
            mjolnir_utility: collect_features
            paths:
                dir_not_exist: !!set
                    ? "%(local_features_data_path)s"
            cmd_args:
                input: "%(hdfs_labeled_data_path)s"
                output-dir: "%(hdfs_features_data_path)s"
                kafka: "%(kafka_brokers)s"
                kafka-request-topic: "%(kafka_request_topic)"
                kafka-result-topic: "%(kafka_result_topic)"
                feature-definitions: "%(feature_definitions)s"
        feature_selection:
            spark_command: "%(SPARK_HOME)s/bin/spark-submit"
            mjolnir_utility_path: "%(mjolnir_utility_path)s"
            mjolnir_utility: feature_selection
            paths:
                dir_not_exist: !!set
                    ? "%(local_pruned_data_path)s"
            spark_conf:
                spark.locality.wait: 0
                spark.dynamicAllocation.maxExecutors: 400
            cmd_args:
                input: "%(hdfs_features_data_path)s"
                num-features: 50
                output-dir: "%(hdfs_pruned_data_path)s"
        make_folds:
            spark_command: "%(SPARK_HOME)s/bin/spark-submit"
            mjolnir_utility_path: "%(mjolnir_utility_path)s"
            mjolnir_utility: make_folds
            spark_conf:
                spark.locality.wait: 0
                spark.dynamicAllocation.maxExecutors: 100
            cmd_args:
                input: "%(hdfs_pruned_data_path)s"
                output-dir: "%(hdfs_fold_data_path)s"
                num-folds: 5
                num-workers: 1
                max-executors: 50
        training_pipeline:
            spark_command: "%(SPARK_HOME)s/bin/spark-submit"
            mjolnir_utility_path: "%(mjolnir_utility_path)s"
            mjolnir_utility: training_pipeline
            paths:
                dir_exist: !!set
                    # TODO: Would be nice if we could specify paths.dir_exist for
                    # input training data, but it's evaluated before data_pipeline is
                    # called when doing collect_and_train.
                    ? "%(base_training_output_dir)s"
            spark_args:
                driver-memory: 3G
            spark_conf:
                # Adjusting up executor idle timeout from 60s to 180s is a bit greedy,
                # but prevents a whole bunch of log spam from spark killing executors
                # between CV runs
                spark.dynamicAllocation.executorIdleTimeout: 180s
            cmd_args:
                input: "%(hdfs_fold_data_path)s"
                output: "%(training_output_dir)s"

# Individual training groups
profiles:
    large:
        # 12M to 30M observations.
        # Approximately 80 executors, 480 cores, 480GB memory
        # 8-11 min per enwiki model @ 6 cores = 48 to 66 cpu minutes
        # 2h40min to final model training
        wikis:
            - enwiki
            - dewiki
        commands:
            make_folds:
                template_vars:
                    # enough space to load datasets into c++
                    executor_memory_overhead: 5120
                cmd_args:
                    num-folds: 3
            training_pipeline:
                template_vars:
                    cores_per_executor: 6
                    cores_per_task: 6
                    #  4096 fails with matrix of 350-400M floats
                    # cv-test-dcg@10: 0.8619
                    executor_memory_overhead: 6144
                spark_conf:
                    spark.dynamicAllocation.maxExecutors: 85
                cmd_args:
                    cv-jobs: 80
                    final-trees: 100

    medium:
        # 4M to 12M observations per executor.
        # Approximately 70 executors, 420 cores, 840GB memory
        wikis:
            - itwiki
            - ptwiki
            - frwiki
            - ruwiki
        commands:
            make_folds:
                template_vars:
                    # enough space to load datasets into c++
                    executor_memory_overhead: 1536
            training_pipeline:
                template_vars:
                    cores_per_executor: 4
                    cores_per_task: 4
                    executor_memory_overhead: 2048
                spark_conf:
                    spark.dynamicAllocation.maxExecutors: 130
                cmd_args:
                    cv-jobs: 125
                    final-trees: 100

    small:
        # 100k to 4M observations per executor. Way overprovisioned
        # Approximately 100 executors, 400 cores, 800G memory.
        wikis:
            - svwiki
            - fawiki
            - idwiki
            - viwiki
            - nowiki
            - hewiki
            - kowiki
            - fiwiki
            - jawiki
            - arwiki
            - nlwiki
            - zhwiki
            - plwiki
        commands:
            make_folds:
                template_vars:
                    # enough space to load datasets into c++
                    executor_memory_overhead: 1024
            training_pipeline:
                template_vars:
                    cores_per_executor: 4
                    cores_per_task: 4
                    executor_memory_overhead: 1024
                spark_conf:
                    spark.dynamicAllocation.maxExecutors: 130
                cmd_args:
                    cv-jobs: 125
                    final-trees: 500
